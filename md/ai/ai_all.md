# 1 : 인공지능 첫걸음

# 인공지능에 대한 오해와 진실

## 인공지능은 로봇이다?

인공지능 = 로봇이란 공식이 항상 성립하지는 않는다.
인공지능이란 하나의 소프트웨어 프로그램이다. 로봇은 그것을 담는 하드웨어 용기이다.

## 인공지능은 스스로 똑똑해질 수 있다?

저절로 혼자 똑똑해질 수는 없다. 
오늘날 인공지능의 90% 이상은 지도학습(supervised learning) 방식으로 훈련된다.
지도학습이란 사람이 데이터 한 건 한 건을 어떻게 판단하거나 처리하는 지에 대한 정답을 인공지능에게 알려주면서 진행하는 방식이다. 

## 인공지능도 감정이 있다?

아니다. 다만 감정을 가지는 척 하도록 프로그래밍 할 수 있다. 
인공지능의 학습 및 추론이란 데이터를 벡터나 매트릭스 형태의 tensor로 만들어 복잡 다양한 행렬 연산을 하는 것에 지나지 않는다.

# 인공지능이란? (Artificial Intelligence)

## 강 인공지능, 약 인공지능

### 강 인공지능

범용성을 가지는 인공지능으로, 범 인공지능이라고도 부른다. (Her, 터미네이터)

### 약 인공지능

일상생활에서 만나는 인공지능이다. 제한된 환경에서 구체적인 특정 업무를 수행하는 데 있어 사람과 비슷한 또는 사람 이상의 성능을 낼 수 있는 인공지능이다. 알파고나 IBM의 암 진단 인공지능이 그 예시이다. 

### 범용성과 전문성

범용성과 전문성의 구별이 절대적이지는 않다. 범용적이라고 해서 다 할 수 있는 것도 아니고, 전문적이라고 해서 하나의 작업만 할 수 있는 것도 아니다. 

## 넓은 의미의 AI, 좁은 의미의 AI

인공지능 > 머신러닝 > 딥러닝

인공지능 : 사람이 해야 할 일을 기계가 대신 할 수 있는 모든 자동화
머신러닝 : 명시적으로 규칙을 프로그래밍 하지 않고 데이터로부터 의사결정을 위한 패턴을 기계가 스스로 학습
딥러닝 : 인공신경망 기반의 모델로, 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한 번에 수행

### 특징 추출과 의사 결정

기계 자동화와 머신러닝, 딥러닝 방식을 구분하는 기준은 두 가지가 있다

- 특징 추출 : 문제 해결을 위해 어떤 정보가 유용할 지, 중요한 것을 꺼내는 과정
- 판단 방식 : 추출한 중요 특징으로부터 인식을 하거나 판단을 하는 모델을 만드는 단계


# 2 : ML/DeepLearning

# 머신러닝과 딥러닝

## 머신러닝이 다루는 정형 데이터(Structured data)

딥러닝이 아닌, 전통적인 머신러닝 기반의 기술들로 다룰 수 있는 데이터를 의미한다. 
**DB, 엑셀, CSV**… 사람이 정제하고 정리한 데이터이다. 

머신러닝은 안정성이 높고 체계적으로 구조가 고정되어있어 유연하지 않은 데이터를 다루는 데에 특화되어 있다. 머신러닝 기반의 기술이란 빅데이터 분석 기법이라고 생각하면 된다. R, SAS, SPSS같은 통계 분석 툴을 활용한다. 

- 선형/로지스틱 회귀분석 : 실수 값 예측에 사용
- 의사 결정 나무 : 카테고리 분류
- ARIMA, ARIMA 모형 : 시계열 예측

## 딥러닝이 다루는 비정형 데이터(Unstructured data)

딥러닝 기반의 AI가 다루는 데이터는 비정형 데이터이다. 
사람이 따로 예쁘게 양식을 정리해놓지 않은, 다양한 형식을 가지는 데이터이다. 
raw data 형태에 가까운 데이터이다. 

- 텍스트 데이터 : 웹 페이지, 상품 리뷰, SNS 글, 기업용 문서, 뉴스 기사
- 음성 데이터 : 전화 통화, 발화, 동영상 내 음성, 기계음, 신호
- 이미지 데이터 : 흑백 사진, 컬러 사진, 손글씨 이미지, 그림, 얼굴 이미지
- 동영상 데이터 : 유투브, 영화, CCTV

# 긍정 리뷰와 부정 리뷰 자동 분류하기

## 특징 및 분류 기준

긍정 키워드, 부정 키워드를 통해 분류한다. 
기계에 우리가 알고 있는 긍정, 부정문에 대한 지식을 알려주는 과정이다. 

## 예외 CASE

특징 및 분류기준으로 분류하는 데에서 예외 케이스 때문에 잘못 분류되는 경우가 많음. 

## 모라벡의 역설(Moravec’s Paradox)

사람에게 쉬운 것이 기계에게는 어렵다. 기계에게 쉬운 것은 사람에게 어렵다. 

## 사람처럼 사고하기

기계에게도 수많은 사례를 통해 구별하게 하면 다양한 예외에도 잘 대처할 수 있지 않을까? 
있는 그대로의 사례들로부터 말로는 할 수 없는, 사전적 정의가 아닌 특징을 배운다, 그것이 사람의 학습 특징이다. 

# 인공신경망 (Artifical Neural Network)

딥러닝이 바로 이런 인간의 사고방식을 기계학습에 녹인 것이다. 

## 인공 뉴런(Artificial Neuron)

생물학적 뉴런의 모양을 그대로 본따서 만든 것. 이전의 뉴런이 넘겨준 데이터를 받아들여 가중합 연산을 한 뒤, 비선형 함수를 적용해 정보를 가공하고 다음에 이어지는 인공 뉴런으로 데이터를 넘긴 것. 

이런 인공 뉴런을 다양한 방식으로 여러 층 쌓아 연결하게 되면 딥러닝의 기본 구조인 인공신경망이 된다. 

딥러닝 모델은 데이터를 통해 자동으로 필요한 특징을 찾아내로 분류를 수행한다. 이전에는 인간이 알고 있는 지식, 규칙을 기계에 전수하려고 했다면, 이제는 인간이 사고하는 방식 그 자체를 기계에게 알려주고 데이터를 제공한다. 

## 딥러닝이 유행하게 된 배경

- GPU의 10배 가량 연산 속도상의 이점을 갖는 TPU(Tensor Processing Unit) 칩이 등장하고 누구나 손바닥만한 외장하드에 몇 테라바이트씩 저장할 수 있는 시대가 되었기 떄문.
- 클라우드 서비스의 발달로 개인의 서버 장비를 구매하지 않아도 필요할 때 필요한 만큼 사용하고 반납할 수 있기 때문.
- 분산 저장 처리
- 빅데이터 처리기술 향상


# 3 : CNN

# 이미지를 인식하는 인공지능

## 인공신경망 연산

딥러닝 모델은 인공 뉴런을 여럿 연결한 인공신경망을 기반으로 하고,
인공 신경망은 가중합과 비선형 함수로 이루어진 연산을 수행해야 한다. 
즉, 입력 데이터로 벡터나 행렬과 같은 형태를 필요로 한다. 그래야 수학 연산이 가능하기 때문이다. 

## 기계가 이미지를 인식하는 방법

인공신경망은 픽셀 값을 가지고 가중합 및 비선형 처리를 다양한 방식으로 수행하여 그림이 무엇인지 인식한다. 

컬러 이미지의 경우에는 2차원의 matrix가 아닌 3차원 tensor를 다룬다. 컬러를 가진 픽셀 하나를 표한하기 위해서 RGB가 필요하다. R, G, B 각각의 값을 나타내는 매트릭스 하나씩 3개, **3 Channel, 3 Depth**가 필요하다. 

**3채널 이미지 : 픽셀 당 RGB 세 개의 값을 갖는 컬러 이미지**

# Convolutional Nerual Network(CNN)

이미지는 가로, 세로의 공간적 정보를 담고 있으니 이를 처리하기 위한 특별한 인공 신경망 구성 방식이 필요하다. 이미지 처리에 특화된 신경망이 CNN이다.

CNN은 아래의 두 가지로 구성되어 있다. 

- Feature Extraction
- 태스크 수행 영역

## 특징 추출(Feature Extraction)

이미지로부터 특징을 추출하는 역할은 Convolution, Pooling 연산이 수행한다. 

**Convolution** : 컨볼루션 연산은 컨볼루션 필터(또는 커널)가 입력 이미지를 상하좌우로 훑으며 **주요한 특징이 있는지 찾아내는 과정**이다. 컨볼루션 필터를 여러개 설정하면 그만큼 이미지로부터 다양한 특징을 찾아낼 수 있다. 

**Feature map / Convolved Feature** : 이렇게 찾아낸 결과 특징이다. 

특징을 찾는 작업은 인공 뉴런이 하는 일과 마찬가지로, 가중합 + 비선형 함수 적용으로 구성된다. 

**Pooling 연산** : 컨볼루션 필터가 이미지를 상하좌우로 훑으며 특징을 찾아낸 뒤, 이 결과로부터 정보를 추리는 풀링연산이 이어진다. 
**풀링 연산은 Feature Map을 상하좌우로 훑으며 핵심 정보만을 영역별로 샘플링**한다. 주로 영역 내 가장 큰 값만 남기고 나머지 값을 버리는 Max Pooling 방식을 적용한다. 

정리하자면, 컨볼루션 연산이 이미지의 특징을 찾아낸다면 풀링 연산은 그 중 핵심 정보만 남기는 것이다. 대부분 이미지 처리 모델에서 컨볼루션과 풀링을 여러 번 반복하며 데이터의 feature를 추린다. 이미지로부터 특징을 배워나가는 작업이라는 뜻에서 이 과정을 **Feature Learning** 이라고 한다. 

## 태스크 수행

이미지로부터 특징을 찾아냈다면 이 정보를 활용하여 목표로 하는 태스크를 수행해야 한다. 아래는 이미지 데이터를 처리하는 대표적인 태스크들이다. 

### Classification

입력으로 받은 이미지를 지정된 K개의 클래스(또는 카테고리) 중 하나로 분류한다. 
강아지 고양이 분류 또는 공장에서 제품 사진을 보고 양호 / 불량을 판별하는 업무 등에 쓰인다. 

### Detection

입력으로 받은 이미지에서 특정 개체가 어디에 위치하는지 x, y 좌표값을 찾아주는 과제이다. 

### Segmentation

픽셀 단위로 영역을 구별해준다. 경계를 찾아야 하기 때문에 detection보다 어렵다. 

# ImageNet Large Scale Visual Recognition Competition

ILSVRC, 이미지 인식 대회.


# 4 : NLU

# 언어를 인식하는 인공지능

## 자연어 이해(NLU: Natural Language Understanding)

### 자연

사람들이 일상적으로 쓰는 언어. 반대 개념은 인공어이다. 인공어는 의도와 목적에 따라 인공적으로 만든 언어이다. 에스페란토, 프로그래밍언어가 이에 해당한다. 

### 언어

말하고 듣는 음성 + 쓰고 읽는 문자

### 이해

처리한다는 것 보다 한 단계 더 높은 수준을 요구한다. 
NLP(자연어 처리)나 NLU(자연어 이해)를 구분 없이 사용하고는 있지만 다르다. NLP 안에 NLU가 포함. 딥러닝 기술의 발달로 경계가 애매해지고 있기는 하다. 

인공신경망에 데이터를 태우려면 데이터의 생김새가 인공 뉴련이 계산(가중합 + 비선형함수) 할 수 있는 형태여야 한다. 

참고 : 음성의 경우 STT(Speech to text) 기술을 활용해 텍스트로 변환 가능하다. 

# 기계에게 사람의 언어를 인식시키려면?

## Tokenizing(Parsing)

한글의 경우 뉴스기사나 논문처럼 글이 잘 정제된 경우 형태로 단위로 잘 쪼개질 수 있지만 채팅이나 SNS의 경우 맞춤법을 잘 지키지 않는 경우가 많아 음절 단위로 쪼개는 편이 좋다. 

## Word embedding

한 덩이의 문장을 파싱 작업을 통해 토큰으로 쪼개면, 인공신경망이 계산할 수 있도록 벡터로 바꾸어야 한다. 토큰을 벡터화 하는 것을 워드 임베딩이라고 한다. 

우리가 읽을 수 있는 토큰(보통 단어 단위)을 다차언의 벡터 공간의 한 점으로 매핑한다. 꽂아 넣는다는 의미에서 워드 임베딩이라는 표현을 사용한다. 

토큰을 벡터화하는 워드 임베딩 기법에는 여러가지가 있다. 

### 원-핫 인코딩

토큰을 벡터화하는 가장 쉽고 직관적인 방법은, 우리가 알고 있는 모든 토큰들을 줄세워 사전을 만들고 순서대로 번호를 붙이는 것이다. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0a15b764-e6b3-4375-8625-3a74a130670c/Untitled.png)

장점 : 간단하다.

단점 : 토큰이 다양하고 수가 많을 수록 토큰 하나를 표현하기 위해서 길이가 아주 긴 벡터를 필요로 한다. 한국어 어휘 표준국어대사전에는 51만개의 단어가 있다고 하는데, 그렇다면 한 단어를 원-핫 인코딩 방식으로 표현하기 위해서는 51만 길이를 갖는 벡터를 활용해야 한다. 

### CBOW와 SKIPGRAM

원-핫 인코딩 방식을 개선하기 위해 벡터 길이도 작으면서 정보도 많이 담긴(dense) 두 가지 방식이 나왔다. 

CBOW, SKIPGRAM 두 방식 중 어떤 방식을 이용하든 먼저 단어(토큰)를 특정 길이를 가진 임의 벡터로 만든다. 이 벡터 길이는 사람이 지정하며, 원-핫 인코딩 방식보다 훨씬 작은 길이이다. 

CBOW - 인공지능에게 문장을 알려주되 중간중간 빈 칸을 만들어 들어갈 단어(토큰)을 유추시킨다.

SKIPGRAM - 인공지능에게 단어(토큰) 하나를 알려주고 주변에 등장할만한 그럴싸한 문맥을 만들도록 시킨다. 

많은 문장을 학습시키면 시킨수록 더 좋은 품질의 벡터가 나온다. 
또, 잘 학습된 워드벡터는 의미 연산이 가능하다. (LG전자 - LG + 삼성 = 삼성전자)

# 다양한 자연어 이해 과제들

## 문장/문서 분류

입력받은 텍스트를 지정된 K개의 클래스(또는 카테고리) 중 하나로 분류하는 과제이다. 
사용자 리뷰를 **감성 분석**(긍/부) 하거나 유저의 발화문을 챗봇이 처리할 수 있는 기능 중 하나로 매핑하는 **의도분류** 등에 쓰일 수 있다. 

## Sequence-to-Sequence

문장/문서를 입력으로 받아 문장을 출력한다. 
번역, 긴 문서를 요약하거나, 자유대화 등 다양한 과제에 활용 가능하다. 

## 질의 응답

사용자 질문이 들어오면 내가 가진 매뉴얼 내에서 가장 답변이 될 가능성이 높은 영역을 리턴하는 MRC(Machine Reading Comprehension)와, 가장 유사한 과거 질문/답변(FAQ)를 꺼내주는 IR(Information Retrieval) 형태가 있다. 

주로 **상담챗봇** 및 **콜센터**에 자주 활용되는 기술이다.



# 5 : RNN, LSTM

# 시간 흐름에 따른 데이터(Sequential data)처리하기

딥러닝 알고리즘의 강점은 비정형 데이터를 규칙 없이도 잘 처리할 수 있다는 점이다. 
매번 일을 반복하면서 쌓인 예측 노하우, 패턴 데이터들을 모두 사용하고 싶어서 나온 것이 바로 RNN이다. 

# Recurrent Neural Network(RNN; 순환 신경망)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/64f3be6e-8145-49fe-9e93-059e56f6646b/Untitled.png)

왼: 입력값으로 출력값을 예측하는 기존의 인공신경망
오: 과거의 처리 이력을 압축해 반영하는 RNN

## 장점

### RNN은 시간 흐름에 따른 과거 정보를 누적할 수 있다

입력데이터 뿐만 아니라 과거의 처리 내역을 반영해 더 나은 결정을 할 수 있다.

### RNN은 가변 길이의 데이터를 처리할 수 있다

단위시간마다의 매 시점을 timestep이라고 한다. 이것은 유저가 구성하기 나름이다. 
RNN은 과거의 정보를 매 timestep마다 압축하여 다음 timestep으로 넘기므로 데이터의 길이에 무관하게 자유롭게 구성할 수 있다. 

### RNN은 다양한 구성의 모델을 만들 수 있다

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/90382fba-4403-44fa-bf8f-80ae04d10f4f/Untitled.png)

유연한 구조를 가졌기 때문에 다양한 구조를 활용하여 신경망을 구성할 수 있다. 
입력 데이터의 정보를 누적하는 부분을 Encoding, 결과를 출력하는 부분을 Decoding이라고 표현한다. 

## 단점

### 연산 속도가 느리다

과거의 처리 내역을 현재에 반영해야 하기 때문에 현 시점의 데이터를 처리하려면 반드시 이전 시점의 데이터가 처리 완료되어야 한다. 그래서 아래의 문제점을 가진다. 

- 병렬 학습이 어렵고
- 연산 속도가 느리다

연산 속도 저하는 텍스트 데이터를 다루는 경우 주로 문제가 된다. 

### 학습이 불안정하다

timestep이 길면 길수록 문제가 발생할 확률이 높다. RNN 인공신경망이 반영해야 할 과거의 이력이 많아지기 때문이다. 이 과정에서 인공신경망이 학습해야 할 값이 폭발적으로 증가하는 현상이 발생할 수 있는데, 이를 **Gradient Exploding**이라고 한다. 

timestep이 짧아지면 멀리 있는 과거의 이력은 현재의 추론에 거의 영향을 미치지 못하는 문제도 생기는데 이를 **Gradient Vanishing**이라고 한다. 

### 실질적으로 과거 정보를 잘 활용할 수 있는 모델이 아니다

먼 과거의 정보를 반영하기 힘들어진다. 왜냐하면 RNN은 한 timestep씩 정보를 누적해 인코딩하는데, 먼 과거의 정보는 여러 번 압축되고 누적되어서 거의 영향을 미치지 못하기 때문이다. 이를 **RNN의 장기 종속성/의존성 문제**(Long term dependency)라고 한다. 

## 성능 보완

### LSTM(Long-short term memory)

이 유닛은 먼 과거의 정보 중 중요한 것은 기억하고, 불필요한 것은 잊어버리도록 스스로 조절한다. 
매번 동일하게 과거의 정보를 누적하고 압축하는 기본 RNN(naive RNN) 유닛과는 다르게 정보 흐름을 잘 조절하기 위해 성능을 개선한 특별한 형태의 뉴런이다. 

LSTM이라는 뜻은 장/단기 메모리 유닛이라는 뜻이다. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ecb01780-44fb-4392-bc46-f9b7d5d0b14e/Untitled.png)

A - Forget gate
잊어버림에 대한 조절. 과거의 정보 중 불필요하다고 생각하는 부분은 통과시키지 않고 잊어버려도 되는 부분을 결정한다. 

B - Input gate
현재의 정보를 얼마나 반영할지 결정. ‘오늘 정보는 별로 안 중요해보이는데? 과거 정보로 충분해!, 거르자!’ 와 같은 결정을 한다. 

C - output gate
현재 시점에 연산된 최종 정보를 다음 시점에 얼마나 넘길지 결정. ‘이건 내일도 쓸 수 있겠다’ 등과 같은 결정을 한다. 

RNN에 비해 더 좋은 예측을 할 수 있게 되나, 연산 속도는 조금 더 느려진다는 단점이 있다. 그래서 이를 조금 개선한 GRU(Gated Recurrent Unit)을 사용하기도 한다. 

### 시계열 데이터

순차적인 흐름을 가지고 진행되는 모든 데이터.

- 10분 간격 timestep에 따라 경북 지역의 일시, 일조량, 풍속, 온도를 가지고 태양관 에너지 발전량 예측
- 기온
- 텍스트 문장. 문장이란 단어가 일정한 순서대로 등장하는 데이터이므로. 한 단어 한 단어를 벡터로 바꿀 수 있으니 RNN의 한 timestep에 단어 벡터를 하나씩 입력시키면 된다. 한국어 문장을 인코딩해서 영어 문장을 디코딩할 수도 있다. (sequence-to-sequence)


# 6 : Offline/Online Process/Overfitting/Generalization

# AI Process

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/607642bf-146c-4b90-b5b3-92b731a3c822/Untitled.png)

## Offline Process

과거에 만들어진 **historical data를 가공하는 것부터 시작**한다. 실시간 데이터가 아닌 DB등에 이미 수집되어 있는 데이터이다. AI 모델러는 기 확보된 데이터를 확인하고 정제해 **필요한 부분을 취하거나 (Generate features) 필요한 경우 레이블을 붙인다. (Collect Labels)**

Label이란 AI 모델 학습을 위해 필요한 정보로, 인공지능이 맞춰야 하는 정답이다.

데이터를 마련하면 **어떤 머신러닝/딥러닝 알고리즘을 활용해 모델을 학습할 것인지** 정하고, 좋은 성능을 달성할 때 까지 **반복 실험**을 진행한다. 

실험 결과를 진단하고 이 때 발견한 문제점을 해결하거나 더 개선된 성능을 낼 수 있도록 실험을 반복하는데 이 과정을 **튜닝**이라고 한다. 튜닝은 모델 학습에 필요한 여러 수치 설정값(하이퍼 파라미터)을 조절하는 등의 역할을 포함한다. 

여러 실험을 반복하며 **개선된 성능의 모델은 배포할 수 있도록 최종 선택(Publish model)**된다. 

이 **과정들을 Traning Pipeline**이라고 한다. 

## Online Process

모델을 개발하는 부분까지가 Offline process이다. 완성된 모델을 고객사 운영 환경에 올린다. 
실전 환경에서 **추론(Inference)** 해야한다. 

온라인 프로세스의 대부분은 머신러닝/딥러닝보다는 개발영역에 가깝다. AI 모델을 운영 환경에 띄운다면(Load model) 나머지는 GUI 를 붙이거나 고객 데이터베이스에 연결하는 등의 일을 한다. 

운영 환경의 스트리밍 데이터(Live Data)를 이제부터 처리하게 된다. AI는 과거의 학습 지식을 통해  현장의 데이터를 추론한다. 

# Overfitting과 Generalization(일반화 성능)

Generalization : 이전에 본 적 없는 데이터에 대해서도 잘 수행하는 능력

Overfitting : 훈련 시에만 잘 작동하고 일반화 성능이 떨어지는 모델

## Training, Validation, Test

확보한 Historical data를 세 개의 set으로 나누고 각 set이 수행할 역할을 구분한다. 
보통은 세 개의 셋을 8:1:1, 6:2:2 정도로 구분한다. 
나누기 전에는 데이터를 골고루 섞어 주어 set 별로 데이터의 성향이 다르거나 치우침이 없도록 한다. 

### Training set

머신러닝/딥러닝 모델을 학습하는 데 이용하는 데이터이다. 모델은 Traning set의 입력 데이터와 정답을 보고 정답을 더 잘 맞추기 위해 노력한다. 단순한 Optimization 이다. 

### Validation set

모델을 튜닝하는 데 도움을 주는 데이터이다. 모델의 일반화 성능을 판단하여 이어질 시험을 계획하는 데 이용한다. 

Traning set은 잘 맞추는데 Validation set에 대해서 너무 못 맞추는 것은 일반화 성능이 부족한 오버피팅 현상이 발생했다고 볼 수 있다. 

### Test set

모델의 학습에 어떤 식으로도 전혀 관여하지 않는 데이터이다. 모델의 최종 성능을 평가하기 위해 따로 뗴어 놓은 데이터이다. 수능 시험과 같은 역할을 한다. (모두가 처음 보는 문제로 동시에 공정하게 평가하는 느낌으로)

## 학습 곡선 확인하기

오버피팅이 발생하지 않았는지 확인한다. 아래는 전형적인 오버피팅의 예시이다. 

Training set의 세로 축은 Cost 인데 이것은 실제 정답과 모델이 예측한 예측값의 차이를 정량화한 수치이다. 작을 수록 모델이 잘 맞춘 것이다. 

Validation set의 세로 축은 Accuracy인데 값이 높을 수록 분류를 잘 한다는 것이나 280 epoch(Traning set 전부를 한 번씩 학습에 이용하는 단위)부터는 큰 개선이 없다. 

Traning set에 대해서만 성능을 개선하고 있고 Validation set에 대해서는 별다른 향상이 없는 시점이 오버피팅 발생 시점이다. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dd204186-2799-4b57-94f3-d658a0b8860c/Untitled.png)

# Regularization

오버피팅을 피하기 위한 모든 전략들을 Regularization이라고 한다. 

## 데이터 증강(Data Augmentation)

데이터를 더 많이 확보하는 것이다. 
데이터를 증강시키기 위해서 이미지를 좌우반전 시키거나, 일부 영역을 크롭하거나, 노이즈 추가하거나, 색상 명암 채도 등에 변화를 주어 데이터를 이용할 수도 있다. 너무 과도한 변형은 해가 된다.

## Capacity 줄이기

Capacity 모델의 복잡한 정도를 나타낸다. 딥러닝 모델이 머신러닝 모델보다 capacity가 높으며 그 중에서도 신경망을 여러층 쌓거나 뉴런의 수를 많이 둘 수록 capacity가 높아진다. 

capacity가 높은 모델은 처리할 데이터의 복잡 다양한 패턴을 더 잘 담아낼 수 있다. 그러나 필요 이상으로 capacity가 높아지는 경우 주어진 데이터를 외우게 될 가능성이 높다. 

## 조기 종료(Early stopping)

오버피팅이 감지될 경우 목표하는 학습 시간이 다 되지 않았다고 하더라도 조기 종료해버리는 것이다. 

## 드롭아웃(Dropout)

Traning pipeline에서 일정 비율 p만큼의 노드(인공 뉴런)를 무작위로 끄고 진행하는 Regularization 기법이다. 일부 노드가 사라진 상태에서 남아있는 노드만으로 어떻게든 정답을 맞춰야 하는 인공지능 모델은 훨씬 강력해진다. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/23c9a79c-5889-4ff2-9b2a-41365929642c/Untitled.png)



# 7 : Transfer Learning

# 쉽지 않은 인공지능 적용하기

## 구체적이지 않으며 불명확한 태스크

## 적은 데이터, 낮은 품질의 데이터

## 다른 도메인 환경

대부분의 딥러닝 모델은 동일한 기능을 수행하는 모델이라고 해도 추론 환경이 달라지면 제 기능을 수행하지 못한다. 때문에 하나의 AI 모델이 좋은 성능을 보인 전적이 있다고 해도 다른 고객사 환경에서 여전히 잘 작동한다는 보장은 할 수 없다. 

# Transfer Leaning : 한 번 만든 인공지능 모델 우려먹기

**전이학습**이라고도 불리는 Transfer Learning은 **한 번 만들어진 딥러닝 모델을 재활용**해 쓸 수 있는 기법이다. 비슷한 태스크를 다른 도메인에 적용할 때, 그리고 그 태스크를 위한 학습 데이터가 부족한 경우 유용하게 쓰일 수 있다. 

Fine-Tuning은 더 정교하게 파라미터를 튜닝하는 것이다. Transfer Learning는 대체로 만들어진 모델을 다른 태스크에 적용하려 할 경우 활용하는 기법이다. Fine-Tuning은 기존 모델의 업데이트 버전을 위한 기법이라고 할 수 있다. 

## Catastrophic forgetting : 치명적인 기억 상실!

딥러닝 모델이 새로운 정보를 학습할 때 이전에 배웠던 정보를 완전히 잊어버리는 경향이 있을 수 있는데, 이를 Catastrophic forgetting이라고 한다. 

Transfer(전이)가 필요 이상으로 과하게 일어났다고 할 수 있다. 

## 더 나은 Transfer Learning을 위한 방법

일반적으로 딥러닝 모델은 두 layer 이상의 인공 신경망으로 구성되어 있다. 이 신경망은 초반에는 데이터의 구체적으로 기본적인 특징을, 후반부로 갈수록 특정 태스크를 위한 추상적이고 개념적인 특징을 학습하게 된다. 

때문에 새 태스크에 기 보유 모델을 Transfer Learning할 땐 기본 특징 학습은 건너 뛰고 바로 태스크를 위한 학습으로 가는 것이 좋다. 이를 위해 후반부의 신경망 층에 대해서만 파라미터 학습하고 전반부의 파라미터는 학습되지 않도록 고정해놓는 기법을 적용할 수 있다. 이를 레이어 동결 (Layer freezing)이라고 한다. 

상황에 맞게 유연하게 Gradual Unfreezing 하거나 대부분 동결시키는 방법을 사용할 수도 있다. 

이 외에 층마다 Learning rate(학습률)의 차별을 두는 방법도 있다. (Discriminative fine-tuning)
Learning rate는 한 번에 인공 신경망 파라미터를 얼마만큼 업데이트 시킬지에 대한 정도로, 사람이 설정하는 hyperparameter값이다. 

주로 전반부의 인공신경망은 새로운 태스크에 적용할 때 Learning rate를 작게 설정하는 것이 좋다. 반면 후반부는 새로 배워나가야 하는 부분이 많을 테니 Learning rate를 크게 설정해 빠르게 학습하는 편이 좋다. 

# Transfer Learning 모델 이용

주로 오픈도메인 데이터에 대해 만들어놓은 모델을 특정 도메인의 태스크에 적용하는 식으로 활용한다. 

## 컴퓨터 비전에서의 Transfer Learning

이미지를 입력데이터로 처리하는 경우라면 이미지넷 데이터로 학습된 모델에 transfer learning을 적용하여 좋은 성능을 낼 수 있다. GoogleNet, ResNet이 대표적이다. 

## 자연어 이해(NLU)에서의 Transfer Learning

방대하고 일반적인 위키같은 곳을 학습한 적이 있는 모델이라면 일반적인 어휘의 의미를 대강 알고 있는 모델이라고 할 수 있다. 따라서 Transfer learning 에 활용하기 좋다.



8 : Pre-trained AI
Pre-training
Transfer Learning을 할 때 전혀 다른 태스크 간에는 지식을 전수하기가 어렵다. 하지만 여러 태스크에 활용하기 위해 여러 지식을 미리 학습해놓은 인공지능을 만들면 되는데, 이를 Pre-traning, 사전학습이라고 한다. 
사전 학습은 보통 특정 데이터타입에 대한 일반적인 지식을 두루 배워놓는 것을 목표로 한다. 
예를 들어 
텍스트 사전학습 모델 : 언어의 일반적인 구조
이미지 사전학습 모델 : 이미지의 일반적인 특징, 색채, 형태
대규모 데이터에 대한 Pre-training
시각 데이터에 대한 사전학습
이미지넷. 120만장의 학습용 이미지를 가지고 1000개 카테고리로 분류하는 과제.
Youtube-8M : 구글이 공개한 동영상 사전학습용 데이터.
언어 데이터에 대한 사전학습
Wikipedia, 나무위키, 국립 국어원의 세종말뭉치
Self-Supervised Learning
대규모로 구할 수 있는 데이터라도 Label까지 잘 달려있는 경우는 드물다. Label은 데이터에 대해 인공지능이 예측하기를 희망하는 결과이다. 
때문에 이 때 활용할 수 있는 학습 방법인 Self-Supervised Learning을 사용한다.

기계가 시스템적으로 자체 label을 만들어서 사용하는 학습 방법이다. 
예 : 이미지 데이터를 위한 Self-Supervised Learning
예 : 텍스트 데이터를 위한 Self-Supervised Learning
예 : Google BERT(Bidirectional Encoder Representations from Transformers)



9 : 족집게 데이터/Active Learning
데이터의 바다, 정보의 홍수
Active Learning: 족집게 데이터로 공부하기
레이블링 할 수 있는 데이터의 수가 제한된 상황에서는 성능 향상에 효과적인 데이터를 선별하는 과정이 중요
Motive : 모델이 잘 맞추기 어려운 데이터를 찾아 학습한다면, 더 적은 훈련시간으로 더 좋은 성능을 낼 수 있을 것이다. 
Objective : 레이블링을 위한 예산이 한정되었을 때, 모델의 성능을 극대화 할 수 있는 Labeling 대상 데이터를 찾기
Active Learning의 절차

Traning a model : 초기 학습 데이터(Labeled Data)를 이용해 모델을 학습한다.
Select Query : 레이블링이 되지 않은 데이터 풀로부터 모델에게 도움이 되는 데이터를 선별한다.
Human Labeling : 선별한 데이터를 사람이 확인해 레이블을 태깅한다.
선별한 레이블 데이터를 기존 학습 데이터와 병합한 후, 다시 모델을 학습한다. 
목표하는 성능이 나올 때 까지 위 방법을 반복해 수행한다. 
Query Strategy
Active Learning의 핵심은 성능 향상에 효과적인 데이터를 선별하는 방법이다. 이런 데이터 선별 방법을 Query Strategy 라고 한다. 
예시 : 모델이 헷갈릴만한 데이터 위주로 추출해 정답을 알려주고 학습한다면 더 정교한 분류 모델이 만들어질 수 있다. 
Uncertainty Sampling
AI 모델은 가장 불확실하다고 생각되는 데이터를 추출해 레이블링이 필요하다고 요청하게 된다. 이런 데이터들을 레이블링해 모델에게 알려주면 분류 성능을 높이는 데 도움이 된다. 
Query by committee
여러 AI 모델 간의 의견 불일치를 종합 고려하는 방식이다. 여러 모델과 추론한 결과 불일치가 많은 데이터일수록 가장 헷갈리는 데이터, 즉 레이블링을 진행할 대상이 된다. 


# 10 : Attention mechanism, XAI

# 긴 입력 데이터 처리하기

긴 입력 데이터를 사람이 읽을 때 전체적으로 원문을 훑으며 중요한 부분을 다시 참고하듯, 인공 신경망 학습에도 이런 모티브를 녹여내고자 함이다. 

# Attention mechanism

RNN은 입력 문장의 단어 하나 하나 누적하여 압축하고 인코딩하고 있다가 모든 문장이 다 들어오면 한 단어씩 번역(디코딩)을 수행한다. 마지막 벡터는 긴 문장을 모두 누적하고 있지만, 문장 앞부분의 내용은 너무 압축되어 거의 내용을 잊어버린 것이나 마찬가지이다. 

여기서 어텐션 매커니즘으로, 번역시에 원문을 재참조하여 현재 디코딩 할 단어와 연관된 중요 부분에 집중케 한다. 

예 ) building 이란 단어를 생성할 때, ‘건물'에 해당하는 단어에 주의를 기울일 필요가 있다고 모델이 판단, 이 단어에 조금 더 집중해 전체 입력을 재조정한 입력 데이터 인코딩 벡터를 만든다. 

## Attention score

중요한 단어에 집중한다는 것이 어텐션 스코어를 계산한다는 것이다. (각 단어에 대한 주의 집중 가중치)

timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0~1 사이의 값이다. 

## Context vector

어텐션 스코어를 구하고 나면 현재 디코딩할 단어와의 관련성을 반영해 다시 입력 문장을 인코딩하게 되는데, 이를 컨텍스트 벡터라고 한다. 

중요한 것은 어텐션 매커니즘이 매번 디코딩마다 직전 단계의 벡터 뿐만 아니라 과거의 모든 데이터의 특징들을 고려한다는 점이다. 딥러닝 모델이 자기 스스로 집중할 영역을 파악한다.

# XAI로서의 어텐션

eXplainable AI : 설명 가능한 인공지능, interpretable AI, 해석 가능한 인공지능

딥러닝 기반의 인공 지능은 일반 ML 이나 전통 룰 기반의 프로그래밍에 비해 예측 정확도는 좋지만, 모델이 너무 복잡하고 해석하지 어렵다. 때문에 민감한 내용을 다루는 도메인에서 문제가 된다. 

## 텍스트에서의 어텐션

모델이 어떤 키워드를 더 집중해서 보고 주제를 분류하는가

## 이미지에서의 어텐션

모델이 단어를 생성할 때 이미지의 어떤 영역을 집중해서 보았는가

# Attention 전성시대, Transformer

Transformer 인공신경망은 입력 데이터끼리의 self-attention을 통해 상호 정보교환을 수행하는 것이 특징이다. 문장 내 단어들이 서로 정보를 파악하며 나와 내 주변 단어간의 관계, 문맥을 더 잘 파악할 수 있게 되는 것이다. 

순차 계산이 필요 없기 때문에 RNN보다 빠르면서도 
맥락 파악을 잘 하고, 
CNN처럼 일부만 보는 것이 아니라 전 영역을 아우른다. 

하지만 이해력이 좋은 대신 모델의 크기가 엄청 커지며
고사양의 하드웨어 스펙을 요구한다.


# 11 : AutoML

# 사람의 손을 필요로 하는 인공지능

튜닝이 필요하다. 튜닝은 현재 실험의 결과 양상을 보고 문제점을 진단하고, AI 모델을 조금 더 나은 방향으로 만들고자 실험을 개선하는 것이다. 여기에는 아키텍처를 변경하거나, 하이퍼 파라미터를 조절하거나 하는 역할이 포함된다. 

# 스스로 진화하는 인공지능, AutoML

적절한 인공지능이 학습되도록 도와주는 기법이다. Automated machine Learning. 자동화된 기계학습이다. AutoML의 역할은 크게 세 가지이다.

1. Feature Engineering 자동화 : AI 모델을 학습하기 위해 데이터로부터 중요한 특징을 선택하고 인코딩하는 방식에 대한 것
2. 하이퍼 파라미터 자동 탐색 : AI 모델 학습에 필요한 사람의 설정들을 의미한다.
3. 아키텍처 탐색 : AI 모델의 구조 자체를 더 효율적인 방향으로 찾아준다.

## 하이퍼파라미터 탐색 자동화

하이퍼 파라미터는 다양한 종류가 있다. 
모델의 파라미터 업데이트를 얼만큼 큰 단위로 할 지 결정하는 학습률(learning rate), 
데이터를 얼마나 쪼개 학습할지 단위인 미니배치 사이즈(mini-batch size), 
데이터를 몇 번 반복 학습할지에 대한 단위 에폭(epoch),… 

사람이 설정해주어야 한다. TensorFlow, PyTorch같은 딥러닝 학습 프레임워크에서는 설정을 디폴트로 제공하고 있지만, 기본 설정으로 학습이 잘 되지 않는다면 하이퍼파라미터를 조금씩 튜닝을 해주어야 한다. 

여러 하이퍼파라미터의 조합을 찾고자 하는 시도가 있었고, 아래는 자주 쓰이는 것 두 가지이다. 

- Grid Search : 최적화 할 하이퍼파라미터의 값 구간을 일정 단위로 나눈 후, 각 단위 조합을 테스트해 가장 높은 성능을 낸 하이퍼 파라미터 조합을 선택하는 방식. 단순하지만 최적화 대상이 되는 하이퍼 파라미터가 많다면 경우의 수가 기하급수적으로 많아져서 탐색에 오랜 시간이 걸린다.
- Random Search : 랜덤하게 하이퍼파라미터의 조합을 테스트하는 방식이다. 그리드 서치에 비해 비교적 빠르게 최적의 조합을 찾아낸다.

**Meta Learner == Learn to Learn == 메타학습**

최근의 AutoML 방식에서는 하이퍼 파라미터도 모델을 통해 탐색한다. - Meta Learner

Meta Learner의 하이퍼 파라미터 조합대로 학습한 Learner의 학습 성능 결과를 Meta Learner로 다시 전달하고, Meta Learner는 이를 개선하기 위한 또 다른 하이퍼 파라미터 조합을 내며, Learner는 이 조합으로 또 다시 학습한다. 이 과정을 반복하면 최적의 조합을 찾을 수 있다. 

## 아키텍처 탐색 자동화

최적의 아키텍처를 찾아주는 방법도 있다. 아키텍처는 모델을 이루는 구조를 말한다. 딥러닝 모델의 경우에는 인공 신경망을 활용하기 때문에 NAS(Neural Architecture search)라고 부른다. 

Learner가 본 과제를 수행하는 AI 모델이라면,

Meta Learner가 어떤 구조의 신경망을 만들면 좋은지, 아키텍처 구성을 고민한다. 
RNN + 강화학습 ← 이와 같은 형식으로 구성해볼 수 있다. 
Learner의 인공신경망 아키텍처가 어떻게 구성되면 좋을지 결정해 Learner의 태스크 수행 결과를 보상으로 활용한다. 

# AutoML 특징

최적의 환경을 알아서 구성해준다. 

# AutoML 서비스

구글 클라우드 플랫폼 : Google AutoML
이미지 분류, 객체 탐지, 동영상 분류, 객체 추적, 자연어 분류, 객체명 인식, 감정 분류, 번역, 정형 테이블 데이터에 대한 회귀, 분류 를 제공하며 이 기능들을 엣지 레벨 (엣지 기기탑재)과 클라우드 레벨 (API형식)으로 제공한다.



# 12 : XAI

# 종종 이해할 수 없는 결정을 내리는 AI

AI는 가끔 인간이 ‘이해할 수 없는’ 결론을 내린다. 

# 설명 가능한 인공지능, XAI(eXplainable Artificial Intelligence)

그래서 이런 인공지능의 한계를 위해, 인공지능에 설명력을 부여하는 연구 분야이다. 

## XAI의 필요성

XAI는 모델에 설명 가능한 근거와 해석력을 부여하여 투명성, 신뢰성을 확보하고자 하는 것이 목적이다. 

# XAI를 위한 접근법

DARPA의 자료에 따르면 AI 모델을 위한 XAI로 크게 세 가지 접근 방식이 있다. 

1. **기존 AI 모델에 설명할 수 있는 어떤 모듈을 덧붙이는 방식**이다. 

## 어텐션 매커니즘(Attention Mechanism)을 활용한 XAI

‘아, 이런 부분이 중요하다고 생각해서 집중한 결과 이렇게 판단했구나' 하는 인사이트를 얻을 수 있다. 

## 설명하는 법 학습하기(Learn to explain)

이 방식은 판단을 하는 딥러닝 모델에 RNN 모듈 등을 덧붙여 인간이 이해할 수 있는 방식의 설명을 생성하도록 하는 방식이다. 

이렇게 만든 모델은 어째서 이런 판단을 결정했는지에 대해 문장을 작성할 수 있다. 하지만 설명 생성에 한계가 있어 널리 활용되지는 않는다. 

- Modular Networks : 딥러닝 모델이 해석 가능한 모듈 구성요소로 이루어진 경우, 판정 결과가 어떤 모듈 경로를 따라 연산되는지 파악하는 모듈러 네트워크(Modular Networks) 방식
- Feature Identification : 딥러닝 모델에서 ‘설명 가능한' 특징을 학습한 노드를 찾아서 그 특징에 설명 레이블을 붙이는 방식

1. 설명력 있는 모델을 만들기
2. 인공 신경망처럼 복잡한 블랙박스 모델의 일부분을 설명해줄 수 있는 다른 모델을 활용하여 유추하기
    1. LIME, SP-LIME 등의 기술이 이에 해당한다.
    2. 일부 영역의 데이터를 활용해 설명력이 좋은 모델을 별도로 하나 더 만들어 학습시키는 방법이다.
        1. 이는 전체 영역을 분류할 수는 없지만 일부 하위 영역을 분류할 수는 있다.


